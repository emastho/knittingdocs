---
title: Prompt token budgeting
description: Trim LLM prompts to fit a token budget before sending them to any model.
hero:
  title: 'LLM input shaping'
sidebar:
  order: 4
---

import { Tabs, TabItem } from '@astrojs/starlight/components';
import { Code } from '@astrojs/starlight/components';
import { getCode } from '../../../../../lib/code-snippets';
import LibraryInstallTabs from '../../../../../components/LibraryInstallTabs.astro';
import RuntimeRunTabs from '../../../../../components/RuntimeRunTabs.astro';

export const run = getCode("data_transforms/prompt_token_budgeting/run_prompt_token_budget.ts");
export const bench = getCode("data_transforms/prompt_token_budgeting/bench_prompt_token_budget.ts");
export const budget = getCode("data_transforms/prompt_token_budgeting/token_budget.ts");

Shapes LLM prompts to fit within a token budget before they hit the API. Uses `tiktoken` for token counting and drops oldest conversation turns first, then trims the query if still over budget. The budgeting logic itself is model-agnostic -- the example uses `gpt-4o-mini` as the tokenizer target, but the pattern works the same way for any model with a token limit.

:::note
This example imports `tiktoken` and `openai`, but it never actually calls the OpenAI API. The `openai` package is optional -- these scripts only prepare prompts and count tokens. The pattern applies to any LLM provider.
:::

## How it works

1. The host generates prompt inputs (same system prefix, different conversation history and queries).
2. Each task builds the full prompt and counts tokens with `tiktoken`.
3. If over budget, it drops the oldest turns first, then trims query tokens.
4. The host aggregates token savings and trim counts.

Three files:

- `run_prompt_token_budget.ts` -- practical prompt-budgeting example for app logic
- `bench_prompt_token_budget.ts` -- dedicated `mitata` benchmark measuring budgeting throughput
- `token_budget.ts` -- the budgeting logic itself

## Install

:::info
<LibraryInstallTabs
  jsrPackages={["@vixeny/knitting"]}
  npmPackages={["tiktoken", "openai", "mitata"]}
  denoUseNpmInterop={true}
/>
:::

`openai` is optional. These examples only prepare prompts and token budgets.
`mitata` is only needed for the benchmark script.

## Run

:::info
<RuntimeRunTabs
  bunCommand="bun src/run_prompt_token_budget.ts --threads 2 --requests 2000 --maxInputTokens 900 --model gpt-4o-mini --mode knitting"
  denoCommand="deno run -A src/run_prompt_token_budget.ts --threads 2 --requests 2000 --maxInputTokens 900 --model gpt-4o-mini --mode knitting"
  nodeCommand="npx tsx src/run_prompt_token_budget.ts --threads 2 --requests 2000 --maxInputTokens 900 --model gpt-4o-mini --mode knitting"
/>
:::

Expected output:

```
mode: knitting (2 threads)
requests: 2000
budget: 900 tokens (gpt-4o-mini)

trimmed: 1,247 / 2,000 requests (62.4%)
avg tokens saved: 312 per trimmed request
total tokens saved: 389,064
elapsed: 1.82s
```

## Benchmark

:::info
<RuntimeRunTabs
  bunCommand="bun src/bench_prompt_token_budget.ts --threads 2 --requests 20000 --maxInputTokens 900 --model gpt-4o-mini --batch 32"
  nodeCommand="npx tsx src/bench_prompt_token_budget.ts --threads 2 --requests 20000 --maxInputTokens 900 --model gpt-4o-mini --batch 32"
/>
:::

Compares budgeting throughput on the host vs through workers. Worker tasks return compact totals (token/trim counters), not full prompt strings.

:::note
Batch size matters here. Tiny batches mostly measure dispatch overhead; very large batches can increase memory pressure. Start with `32` or `64`, then tune for your hardware.
:::

## Code

<Tabs>
  <TabItem label="run_prompt_token_budget.ts">
    <Code code={run} lang="ts" title={"run_prompt_token_budget.ts"} />
  </TabItem>
  <TabItem label="bench_prompt_token_budget.ts">
    <Code code={bench} lang="ts" title={"bench_prompt_token_budget.ts"} />
  </TabItem>
  <TabItem label="token_budget.ts">
    <Code code={budget} lang="ts" title={"token_budget.ts"} />
  </TabItem>
</Tabs>

## When this matters

Token budgeting is a preflight step that runs on every LLM request. If you're handling high-throughput chat traffic -- multiple users, long conversation histories -- the tokenization and trimming work adds up. Offloading it to workers keeps your main thread focused on routing and I/O while budget calculations happen in parallel. It also gives you predictable input sizes, which helps with cost control and latency.