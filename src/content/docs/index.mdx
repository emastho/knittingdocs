---
title: "Knitting"
description: "Knitting is shared-memory IPC for Node.js, Deno, and Bun. Build worker pools where tasks feel like normal function calls."
template: splash
head:
  - tag: meta
    attrs:
      name: keywords
      content: "shared-memory IPC, worker pools, worker threads, Node.js, Deno, Bun, parallel JavaScript, Knitting"
  - tag: meta
    attrs:
      property: og:type
      content: website
  - tag: meta
    attrs:
      name: robots
      content: "index,follow,max-image-preview:large"

hero:
  image:
    alt: A glittering, brightly colored logo
    dark: ../../assets/logo.svg
    light: ../../assets/light_logo.svg
  tagline: Shared-memory worker pools for Node.js, Deno, and Bun -- function-call ergonomics on the shared-memory fast path.
  actions:
    - text: Quick Start
      link: /start/quick-start/
    - text: Benchmarks
      link: /benchmarks/introduction/
    - text: Examples
      link: /examples/intro_examples/
---

import AshesBackground from "../../components/AshesBackground.astro";
import KnittingArchitectureDiagram from "../../components/KnittingArchitectureDiagram.astro";
import {
  Card,
  CardGrid,
  FileTree,
  LinkCard,
  Tabs,
  TabItem,
} from "@astrojs/starlight/components";
import nodeIpc from "../../assets/charts/node_ipc.png";
import denoIpc from "../../assets/charts/deno_ipc.png";
import bunIpc from "../../assets/charts/bun_ipc.png";
import BenchmarkBalls from "../../components/BenchmarkBalls.astro";

<AshesBackground />

<BenchmarkBalls
  sourceLabel="based on node.js ipc graph (50 msg) rountrip of a JSON 12 Bytes"
  items={[
    { label: "Knitting", speed: 2030869, color: "#336244ff" },
    { label: "PostMessage", speed: 573394, color: "#58523bff" },
    { label: "WebSockets", speed: 178603, color: "#A78BFA" },
    { label: "HTTP", speed: 17483, color: "#60A5FA" },
  ]}
/>

Knitting lets you run tasks across worker threads as if they were normal function calls. Under the hood it uses shared memory to keep coordination fast, and it works on Node.js, Deno, and Bun. Curious how it actually works? Read the architecture section below, then check the benchmarks to see it in action.

## Where do you want to start?

<CardGrid>
  <LinkCard
    title="Quick start"
    href="/start/quick-start/"
    description="Up and running in a few lines of code."
  />
  <LinkCard
    title="How Knitting works"
    href="#how-knitting-works"
    description="What's actually happening under the hood."
  />
  <LinkCard
    title="Creating pools"
    href="/guides/creating-pools/"
    description="Pick your thread count, balancer, and more."
  />
  <LinkCard
    title="Supported payloads"
    href="/guides/payloads/"
    description="What you can send to workers and how it gets there."
  />
  <LinkCard
    title="Permissions"
    href="/guides/permissions/"
    description="Restrict what workers can access per runtime."
  />
  <LinkCard
    title="Performance model"
    href="/guides/performance/"
    description="Where the speed comes from and how to get the most out of it."
  />
  <LinkCard
    title="Examples"
    href="/examples/intro_examples/"
    description="Real patterns you can copy -- SSR, validation, servers, and more."
  />
</CardGrid>

## Highlights

<div class="home-cards">
  <CardGrid stagger>
    <Card title="Multi-threading in 10 lines">
      This is the whole thing. Define a task, spin up a pool, call it, done.

      ```ts
      import { isMain, task } from "@vixeny/knitting";

      export const world = task({
        f: (args: string) => args + " world",
      }).createPool({
        threads: 2,
      });

      if (isMain) {
        world.call("hello")
          .then(console.log)
          .finally(world.shutdown);
      }
      ```


      That's it. Batching, balancing, and all the fancy stuff comes later if you need it.
    </Card>

    <Card title="Throughput">
      How fast can you move data? We measured a full `1 MB` echo roundtrip -- host sends `1 MB` to a worker, worker sends `1 MB` back. That's `2 MB` per call.

      **Here's what we saw (GB/s, one-way equivalent):**

      | Runtime | `Uint8Array` | `string` |
      | --- | ---: | ---: |
      | **Bun** | **11.93** | **9.74** |
      | **Deno** | **1.17** | **2.68** |
      | **Node** | **1.15** | **1.32** |

      We report one-way equivalent (half the roundtrip) to keep things honest.

      [See the full benchmarks](/benchmarks/introduction/)
    </Card>

    <Card title="How fast?">
      Same benchmark, three runtimes. Flip the tabs to compare.

      <Tabs>
        <TabItem label="Node" icon="node">
          <img
            src={nodeIpc.src}
            width={nodeIpc.width}
            height={nodeIpc.height}
            alt="Node IPC benchmark"
            loading="lazy"
          />
        </TabItem>
        <TabItem label="Bun" icon="bun">
          <img
            src={bunIpc.src}
            width={bunIpc.width}
            height={bunIpc.height}
            alt="Bun IPC benchmark"
            loading="lazy"
          />
        </TabItem>
        <TabItem label="Deno" icon="deno">
          <img
            src={denoIpc.src}
            width={denoIpc.width}
            height={denoIpc.height}
            alt="Deno IPC benchmark"
            loading="lazy"
          />
        </TabItem>
      </Tabs>

      These give you the shape of it -- dig into the benchmark pages for the full story.
    </Card>





    <Card title="One more thread, big impact">
      The easiest win? Take the heavy stuff -- SSR, parsing, crypto, compression -- and toss it into a worker pool. Your main thread stays free to handle requests.

      In the Hono server example we saw:
      - Throughput: `/ping` **2998 → 5471 (+82%)**, `/ssr` **2998 → 5421 (+81%)**, `/jwt` **2916 → 5454 (+87%)** — roughly **1.8× across the board**.
      - Latency: median and `p99` roughly halve for all routes, and the JWT tail (`p99.9`) falls by **~48%**, so even the slowest hops get faster.

      It's simple: offloading expensive work keeps the main thread free, so cheap routes keep flowing while heavy work runs in workers.

      Try it yourself:
      - [Hono + SSR + JWT + Zod](/examples/data_transforms/rendering_output/hono_server)
    </Card>

    <Card title="10+ examples (and growing)">
      Not toy snippets -- real patterns you can actually copy into your project.
      We add more as new workflows come up.

      <FileTree>
      - examples/
        - intro_examples.mdx
        - data_transforms/
          - intro_data_transforms.mdx
          - rendering_output/
            - React_ssr.mdx
            - Hono_server.mdx
            - ...
          - validation/
            - Schema_validate.mdx
            - Jwt_revalidation.mdx
            - ...
        - maths/
          - Big_prime.mdx
          - Monte_pi.mdx
          - Physics_loop.mdx
          - ...
      </FileTree>

      - [Browse examples](/examples/intro_examples)

      Guides and benchmarks live in the sidebar. Examples are ready to copy and paste.
    </Card>

    <Card title="Flexible by default">
      Works the way you'd expect -- promises in, async out, no surprises.

      - Payload rules are explicit, so you always know what's being serialized.
      - Batching and simple payloads get you the best throughput.

      **And the production knobs are there when you need them:**

      - `balancer` strategies for lane selection.
      - `inliner` to run tasks on the host when it makes sense.
      - `permission` to restrict what workers can access (strict/unsafe).
      - Abort signals to cancel in-flight work.
      - `spin`/`park`/`backoff`/`timeout` for fine-tuning.
      - See: [Creating pools](/guides/creating-pools/), [Permissions](/guides/permissions/), [Performance model](/guides/performance/)

    </Card>

  </CardGrid>
</div>


## How Knitting Works

<KnittingArchitectureDiagram caption="Shared-memory request/response mailboxes keep the control path cheap; payload buffers and batching keep the data path fast." />

The core idea: instead of sending messages through the runtime's queue, Knitting coordinates through shared memory. That cuts out the middleman and makes task calls cheap.

<CardGrid>
  <Card title="Control path: shared mailboxes">
    There are two mailboxes -- one for requests, one for responses. Think of it as a full-duplex pipe in shared memory.

    - A call claims a slot, writes a small header, and wakes up a worker.
    - The worker does the work, writes the result back, and wakes up the host.
    - Slot ownership uses bitsets, so it's all lock-free.
  </Card>

  <Card title="Data path: payload buffers">
    When values are too big for the header, they go into reusable payload buffers.

    - Each worker gets two `SharedArrayBuffer`s -- one for requests, one for returns.
    - You control the sizes (`payloadInitialBytes`, `payloadMaxBytes`), so you can trade memory for speed.
    - Full details: [Supported payloads](/guides/payloads/).
  </Card>

  <Card title="Worker runtime: the spin-park-wake loop">
    Each worker is a real OS thread spawned via the runtime's native worker API (`worker_threads` on Node, `Worker` on Deno/Bun). When a worker starts, it:

    1. **Boots and loads tasks** -- imports your task modules from the URL list passed at spawn time, building an internal function table indexed by task ID.
    2. **Signals ready** -- writes a status flag into shared memory so the host knows this lane is live.
    3. **Enters the main loop** -- a tight cycle that checks for work, executes it, and writes results back. The loop runs three phases per iteration:
       - *Enqueue* -- reads new request slots from the shared mailbox.
       - *Service* -- executes pending tasks (sync or async) in batches of up to 32.
       - *Write-back* -- flushes completed results into the response mailbox so the host can resolve promises.
    4. **Sleeps efficiently** -- when there's no work, the worker spins briefly (configurable via `spinMicroseconds`). If nothing arrives, it triggers a garbage collection (`global.gc()`) -- using idle time productively instead of just spinning. Then it parks using `Atomics.wait` (a futex-style block) until the host writes new work and wakes it. No CPU burn while idle.

    Async tasks (functions that return a promise) are awaited inside the worker before the result is written back -- there are no half-resolved surprises crossing the thread boundary.
  </Card>

</CardGrid>

Want to understand the cost model and when to batch? That's all in the [Performance model](/guides/performance/).

## When Knitting Helps (and When It Doesn't)

<CardGrid>
  <Card title="Good fit">
    - CPU-heavy or bursty work (SSR, parsing, validation, crypto, compression).
    - Lots of small-to-medium calls where IPC overhead adds up.
    - You care about tail latency and don't want one hot request blocking everything.
    - You can batch calls to squeeze out more throughput.
  </Card>

  <Card title="Not the right tool">
    - Mostly I/O-bound work (waiting on the network or a database).
    - Calls are rare, or each job is so big that coordination cost doesn't matter.
    - You need a general-purpose message bus -- use `postMessage` / MessagePort for that.
    - You need to run across machines (Knitting is local, in-process only).
  </Card>
</CardGrid>

## How Knitting Compares

It's not "workers, but faster" -- it's "task execution, but way cheaper to coordinate."

| | Knitting | `postMessage` / MessagePort | `child_process` / `cluster` |
| --- | --- | --- | --- |
| Primary goal | Low-overhead task calls | General messaging | Isolation + scaling out |
| Typical API | `call.*()` promises | Custom protocol | Custom protocol |
| Coordination path | Shared memory + wakeups | Message queue | OS IPC |
| Best fit | High-frequency CPU work | Event-style messages | Multi-process services |

The benchmarks show what this difference looks like in practice: [Benchmarks overview](/benchmarks/introduction/).

## Safety & correctness

Here's how Knitting behaves so you're not guessing:

- **Async is deterministic:** promises resolve on the host *before* dispatch, and async results are awaited in the worker *before* returning. No half-resolved surprises on either side.
- **Errors stay contained:** if a task throws or rejects, that single call fails -- the worker keeps going for the next one. One bad call doesn't take down the pool.
- **No busy-waiting by default:** idle workers spin briefly, trigger a GC if no work comes (so idle time is productive), then park until there's work. The host dispatcher backs off too. Your CPU isn't burning cycles on nothing.
- **Fair by default:** round-robin lane selection keeps things balanced. But Knitting won't preempt a long-running task -- if one call takes forever, that lane is busy until it's done.
- **Functions aren't payloads:** you can't send a function to a worker. Knitting catches that early and fails fast before dispatch.

:::caution
Knitting is not a security boundary. Tasks run in your process with your permissions. The [permission protocol](/guides/permissions/) restricts file-system and sub-capability access, but if you need to run untrusted code, use a container or VM for real isolation.
:::

## Runtime support

- Works on **Node.js**, **Deno**, and **Bun**.
- No browser or Web Worker support -- that's intentional, not a missing feature.
- Edge runtimes aren't targeted right now.
- Watch out for remote imports and `href` overrides -- they widen your trust boundary. Avoid in production unless you know what you're doing.

## Why Knitting

Worker APIs are great for message passing. Knitting takes a different bet: **what if parallel code was cheap enough that you'd actually use it?**

Instead of building message protocols, you write function calls. Instead of paying for runtime IPC on every hop, you coordinate through shared memory. Same API on Node, Deno, and Bun.

- Call functions instead of routing messages.
- Less IPC overhead, especially for high-frequency workloads.
- One programming model across all three runtimes.
- You control batching, inlining, and shutdown when you need to.

## Scope

Knitting does one thing: local multi-thread execution with shared-memory IPC. It's not a distributed scheduler or a cluster manager.
